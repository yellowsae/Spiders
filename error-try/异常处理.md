当爬取网页时， 出现下一页下一页的，但是不知道一个内容中有多少页的内容。所以我想到的两种思路

想到的方法一 : 使用While  True 的方法： 

```py
def main(url,title):
    # url = 'https://www.tujigu.com/a/41553/'
    base_url = [] 
    base_url.append(url)
    cont = 1  # 索引 
    # 使用 While 循环，进行下一页的爬取 
    while url:
        html = get_url(url)
        url, img_urls = get_html(html.text,url)
        base_url.append(url)

        # 数据下载保存 
        for info_url in img_urls:
            data = get_url(info_url)
            name = info_url.split('/')[-1]
            parse(data.content,name,title)
        cont +=1 
        # 判断是否相同，如果相同就停止循环 
        if base_url[cont-2] == base_url[cont-1]:
            # 删除最后一个元素，因为最后一个是重复的
            base_url.pop() 
            break
```

但是这个方法， 需要访问的次数太多次了， 需要执行的操作也很多，太繁琐，太复杂了， 速度也很慢 。





![image-20210620235900948](https://gitee.com/yunhai0644/imghub/raw/master/image-20210620235900948.png)

方法二： request 对url 的判断是否有误

因为在网页中的下一页url ，一般是和当前页面的url 相关，比如后面的url 数字递增，所以对url进行循环翻页的操作，判断最后的url 是否出错，出错就 停止 。

```py
import requests
from fake_useragent import UserAgent 
from parsel import Selector
from requests import ConnectionError, ReadTimeout
from time import sleep


def get_url(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS)"
    }
    ip = '127.0.0.1:10809'
    proxies = {
        'http':'http://' + ip,
        'https':'https://' + ip
    }
    try:
        sleep(0.1)
        response = requests.get(url,headers=headers,proxies=proxies)
        if response.status_code == 200:
            return response.status_code
        else:
            print(" Get page Failed ",response.status_code)
            return None
    except (ConnectionError, ReadTimeout):
        return None


def main():
    for page in range(2,20):
        url = f'https://www.tujigu.com/a/41553/{page}.html'
        html = get_url(url)
        print(html,url)


if __name__ == '__main__':
    main()
```



可以使用 While  True 判断  start_code 的码， 200 、  404   获取网页内容 

![image-20210621003005899](https://gitee.com/yunhai0644/imghub/raw/master/image-20210621003005899.png)